<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Taekyung Ki</title>

    <meta name="author" content="Taekyung Ki">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="assets/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Taekyung Ki
                </p>
                <p>
                  I am a researcher at <a href="https://www.mlai-kaist.com">KAIST MLAI</a>, working on generative models and computer vision.
                  From March 2022 to March 2025, I conducted research on video generation as part of my mandatory military service in South Korea.
                  I began working in deep learning in February 2021.
                  I received my M.S. in Mathematics in February 2021 and my B.S. in Mathematics in February 2019.
                </p>
                <p> I am interested in the following research topics:
                  <ul>
                      <li>Generative models (<i>score models, flow models, one-step models</i>, etc.)</li>
                      <li>Interactive visual agentic avatar </li>
                      <li>Video generation</li>
                      <li>Audio-visual and vision-language representation</li>
                  </ul>
              </p>                    
              <p>
                  If you have an interest in the above topics, feel free to drop me an e-mail.
              </p>
                <p style="text-align:center">
                  <a href="mailto:taekyung.ki@icloud.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BNX2q6IAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/taekyungki">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/TaekyungKi">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/tkkitkki">Hugging Face</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/taekyung-ki-736220217">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="assets/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 0%;" alt="profile photo" src="assets/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <h2>Highlights</h2>
                <ul>
                  <li>[<strong>June 2025</strong>] One paper accepted to <a href="https://iccv.thecvf.com" target="_blank">ICCV 2025</a>!</li>
                  <li>[<strong>June 2025</strong>] New preprint out. <a href="https://frame-guidance-video.github.io">Frame Guidance</a>; Training-free frame-level guidance method for large video diffusion models
                  <li>[<strong>Mar. 2025</strong>] Joined <a href="https://www.mlai-kaist.com">KAIST MLAI</a> as a researcher
                  <li>[<strong>July 2024</strong>] One paper accepted to <a href="https://eccv2024.ecva.net/Conferences/2024" target="_blank">ECCV 2024</a></li>
                  <li>[<strong>Mar. 2022 - Mar. 2025</strong>] Mandatory military service for South Korea</li>
              </ul>
              </td>
            </tr>
          </tbody>
        </table>
<br><br>
<table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <h2>Publications</h2>
  <p> &ast;: Equal contribution, &dagger;: Corresponding author
    <tr style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/frameguidance.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://frame-guidance-video.github.io">
              <span class="papertitle">Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models</span>
            </a>
            <br>
            <a href="https://agwmon.github.io">Sangwon Jang<sup>&ast;</sup></a>,
            <strong>Taekyung Ki</strong><sup>&ast;</sup>, 
            <a href="https://harryjo97.github.io"> Jaehyeong Jo</a>,
            <a href="https://jaehong31.github.io"> Jaehong Yoon</a>,
            <a href="https://sites.google.com/view/sooyekim"> Soo Ye Kim</a>,
            <a href="https://sites.google.com/site/zhelin625/"> Zhe Lin</a>,
            <a href="http://www.sungjuhwang.com"> Sung Ju Hwang<sup>&dagger;</sup></a>
            <br>
            <em>arXiv</em>, 2025
            <br>
            <a href="https://frame-guidance-video.github.io">Project Page</a> /
            <a href="https://arxiv.org/abs/2506.07177">arXiv</a> /
            <a href="https://github.com/agwmon/frame-guidance">Code</a> /
            <a href="https://huggingface.co/papers/2506.07177">Hugging Face</a>            
          </div>
          <p> Frame Guidance is a training-free guidance method for large-scale VDMs, which enables to generate frame-level controllable videos only using a single GPU.</p>
        </div>
      </td>
    </tr>

    <tr bgcolor="#ffffd0" style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/float.gif" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          <div>
            <a href="https://deepbrainai-research.github.io/float/">
              <span class="papertitle">FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</span>
            </a>
            <br>
            <strong>Taekyung Ki</strong>,
            <a href="https://kevinmin95.github.io">Dongchan Min</a>,
            Gyeongsu Chae
            <br>
            <em>ICCV</em>, 2025
            <br>
            <a href="https://deepbrainai-research.github.io/float/">Project Page</a> /
            <a href="https://arxiv.org/abs/2412.01064">arXiv</a> /
            <a href="https://github.com/deepbrainai-research/float">Code</a> / 
            <a href="https://huggingface.co/papers/2412.01064">Hugging Face </a>            
            <p>FLOAT is a motion latent flow matching model for audio-driven talking portrait generation and editing using its learned orthonormal motion basis.</p>
          </div>
        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/export3d.gif" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:5px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://export3d.github.io">
              <span class="papertitle">Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation</span>
            </a>
            <br>
            <strong>Taekyung Ki</strong>,
            <a href="https://kevinmin95.github.io">Dongchan Min</a>,
            Gyeongsu Chae
            <br>
            <em>ECCV</em>, 2024
            <br>
            <a href="https://export3d.github.io">Project Page</a> /
            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00159.pdf"> Paper</a> /
            <a href="https://arxiv.org/abs/2404.00636">arXiv</a> /
            <a href="https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00159-supp.pdf">Supp</a>
            <p>We propose a contrastive pre-training framework for appearance-free facial expression hidden in 3DMM and 3D-aware, expression-controllable portrait animation model.</p>
          </div>
        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/stylelipsync.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://stylelipsync.github.io">
              <span class="papertitle">StyleLipSync: Style-based Personalized Lip-sync Video Generation</span>
            </a>
            <br>
            <strong>Taekyung Ki<sup>&ast;</sup></strong>, 
            <a href="https://kevinmin95.github.io">Dongchan Min<sup>&ast;</sup></a>
            <br>
            <em>ICCV</em>, 2023
            <br>
            <a href="https://stylelipsync.github.io">Project Page</a> /
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ki_StyleLipSync_Style-based_Personalized_Lip-sync_Video_Generation_ICCV_2023_paper.pdf">Paper</a> /
            <a href="https://arxiv.org/abs/2305.00521">arXiv</a> /
            <a href="https://github.com/TaekyungKi/StyleLipSync">Code</a> /
            <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Ki_StyleLipSync_Style-based_Personalized_ICCV_2023_supplemental.pdf">Supp</a>
            <p>StyleLipSync can generate person-agnostic audio-lip synchronized videos by leveraging the strong facial prior of style-based generator.</p>
          </div>

        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:32%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/scattering.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:68%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://stylelipsync.github.io">
              <span class="papertitle">Deep Scattering Network with Max-pooling</span>
            </a>
            <br>
            <strong>Taekyung Ki</strong>, 
            <a href="http://wavelets.yonsei.ac.kr/~wavelets/index.html">Youngmi Hur<sup>&dagger;</sup></a>
            <br>
            <em>DCC</em>, 2021
            <br>
            <a href="https://arxiv.org/abs/2101.02321">Paper</a>
            /
            <a href="https://github.com/TaekyungKi/Scattering_maxp">Code</a>
            <p> We mathematically prove that the pooling operator is a crucial component for translation-invariant feature extraction in Scattering Network.</p>
          </div>

        </div>
      </td>
    </tr>
  </tbody></table>
<br>
<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
<tbody>
<!--  -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
      <td style="padding:0px;width:100%;vertical-align:middle">
        <h2>Awards and Honors</h2>
        <ul>
          <li>[Oct. 2021] 1st Prize Winner, NLP-based Math-Word Problem Track in 2021 AI Grand Challenge (AGC), funded by <a href='https://www.msit.go.kr/eng/index.do'>South Korea Ministry of Science and ICT</a>.</li>
          <li>[Feb. 2019] Graduated with top honors, ranked 1st in the Department of Mathematics.</li>
          </ul>
      </td>
    </tr>
  </tbody>
  </table>

<br><br>

<!--  -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
    <td style="padding:0px;width:100%;vertical-align:middle">
      <h2>Academic Services</h2>
      <ul>
        <li>Reviewer: CVPR 2025, ICCV 2025.</li>
    </ul>
    </td>
  </tr>
</tbody>
</table>

</tbody>
</table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This page is based on Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">website template</a>.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
