<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Taekyung Ki</title>

    <meta name="author" content="Taekyung Ki">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="assets/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Taekyung Ki
                </p>
                <p>
                  I am a 1st-year Ph.D. student at <a href="https://www.mlai-kaist.com">KAIST MLAI</a> advised by Prof. <a href='http://www.sungjuhwang.com'>Sung Ju Hwang</a>. 
                  From March 2022 to March 2025, I conducted research on video generation as part of my mandatory military service in South Korea. 
                  I began working in deep learning in February 2021. I received my M.S. in Mathematics in February 2021 and my B.S. in Mathematics in February 2019.
                </p>
                <p> I am interested in the following research topics:
                  <ul>
                      <li>Generative models (diffusion, flow, autoregressive models etc.)</li>
                      <li>Interactive video generative models</li>
                      <!-- <li>Physics-aware and reliable video generation</li>
                      <li>Efficient post-training (distillation, rl, etc.)</li> -->
                  </ul>
              </p>
              <p> I am <b>open to research collaborations globally</b>! If the topics above align with your research, feel free to contact me an e-mail. </p>                    
              </p>
                <p style="text-align:center">
                  <a href="mailto:taekyung.ki@kaist.ac.kr">Email</a> &nbsp;/&nbsp;
                  <a href="assets/cv.pdf"> CV </a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BNX2q6IAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/taekyungki">X</a> &nbsp;/&nbsp;
                  <a href="https://github.com/TaekyungKi">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/taekyungki">Hugging Face</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/taekyung-ki-736220217">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:20%;max-width:20%">
                <a href="assets/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 0%;" alt="profile photo" src="assets/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <h2>Highlights</h2>
                <ul>
                  <li>[<strong>Feb. 2026</strong>] One paper is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2026</a>
                  <li>[<strong>Jan. 2026</strong>] One paper is accepted to <a href="https://iclr.cc/">ICLR 2026</a>
                  <li>[<strong>Jan. 2026</strong>] New preprint out. <a href="https://agwmon.github.io/self-refine-video/"> Self-refining video sampling</a> for enhancing motion and physics</li>
                  <li>[<strong>Jan. 2026</strong>] New preprint out. <a href="https://taekyungki.github.io/AvatarForcing/">Avatar Forcing</a>; Motion latent diffusion forcing for interactive head avatar</li>
                  <li>[<strong>Sep. 2025</strong>] Starting Ph.D. program at <a href='https://gsai.kaist.ac.kr'>KAIST AI</a></li>
                  <li>[<strong>June 2025</strong>] One paper is accepted to <a href="https://iccv.thecvf.com" target="_blank">ICCV 2025</a></li>
                  <li>[<strong>Mar. 2025</strong>] Joined <a href="https://www.mlai-kaist.com">KAIST MLAI</a> as a researcher
                  <li>[<strong>July 2024</strong>] One paper is accepted to <a href="https://eccv2024.ecva.net/Conferences/2024" target="_blank">ECCV 2024</a></li>
                  <li>[<strong>Mar. 2022 - Mar. 2025</strong>] Mandatory military service for South Korea</li>
              </ul>
              </td>
            </tr>
          </tbody>
        </table>
<br><br>
<table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
  <h2>Publications</h2>
  <p> &ast;: Equal contribution, &dagger;: Corresponding author
    <tr bgcolor="#ffffd0" style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/pnp.gif" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          <div>
            <a href="https://agwmon.github.io/self-refine-video/">
              <span class="papertitle">Self-Refining Video Sampling</span>
            </a>
            <br>
            <a href="https://agwmon.github.io">Sangwon Jang<sup>&ast;</sup></a>,
            <strong>Taekyung Ki<sup>&ast;</sup></strong>,
            <a href="https://harryjo97.github.io"> Jaehyeong Jo<sup>&ast;</sup></a>, 
            <a href="https://www.sainingxie.com/"> Saining Xie</a>,
            <a href="https://jaehong31.github.io"> Jaehong Yoon<sup>&dagger;</sup></a>, 
            <a href="http://www.sungjuhwang.com"> Sung Ju Hwang<sup>&dagger;</sup></a>
            <br>
            arXiv 2026
            <br>
            <a href="https://agwmon.github.io/self-refine-video/">Project Page</a> /
            <a href="https://arxiv.org/abs/2601.18577">arXiv</a> / 
            <a href="https://github.com/agwmon/self-refine-video">Code</a> /
            <a href="https://huggingface.co/papers/2601.18577">Hugging Face</a>
            <p>We propose a self-refining video sampling method that achieves better motion-enhanced video generation without additional training or external verifiers.</p>

          </div>
        </div>
      </td>
    </tr>

    <tr bgcolor="#ffffd0" style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/avatarforcing.gif" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          <div>
            <a href="https://taekyungki.github.io/AvatarForcing/">
              <span class="papertitle">Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</span>
            </a>
            <br>
            <strong>Taekyung Ki<sup>&ast;</sup></strong>,
            <a href="https://agwmon.github.io">Sangwon Jang<sup>&ast;</sup></a>,
            <a href="https://harryjo97.github.io"> Jaehyeong Jo</a>, 
            <a href="https://jaehong31.github.io"> Jaehong Yoon</a>, 
            <a href="http://www.sungjuhwang.com"> Sung Ju Hwang<sup>&dagger;</sup></a>
            <br>
            Conference on Computer Vision and Pattern Recognition <strong style="color: navy;"><em>(CVPR)</em></strong>, 2026
            <br>
            <a href="https://taekyungki.github.io/AvatarForcing/">Project Page</a> /
            <a href="https://arxiv.org/abs/2601.00664">arXiv</a> / 
            <a href="https://github.com/TaekyungKi/AvatarForcing">Code</a> /
            <a href="https://huggingface.co/papers/2601.00664">Hugging Face</a>
            <p>We introduce a user-interactive head generation model based on motion latent diffusion forcing and a direct preference optimization method for interactive motion.</p>
          </div>
        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/frameguidance.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://frame-guidance-video.github.io">
              <span class="papertitle">Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models</span>
            </a>
            <br>
            <a href="https://agwmon.github.io">Sangwon Jang<sup>&ast;</sup></a>,
            <strong>Taekyung Ki</strong><sup>&ast;</sup>, 
            <a href="https://harryjo97.github.io"> Jaehyeong Jo</a>,
            <a href="https://jaehong31.github.io"> Jaehong Yoon</a>,
            <a href="https://sites.google.com/view/sooyekim"> Soo Ye Kim</a>,
            <a href="https://sites.google.com/site/zhelin625/"> Zhe Lin</a>,
            <a href="http://www.sungjuhwang.com"> Sung Ju Hwang<sup>&dagger;</sup></a>
            <br>
            International Conference on Learning Representations <strong style="color: navy;"><em>(ICLR)</em></strong>, 2026
            <br>
            <a href="https://frame-guidance-video.github.io">Project Page</a> /
            <a href="https://arxiv.org/abs/2506.07177">arXiv</a> /
            <a href="https://github.com/agwmon/frame-guidance">Code</a> /
            <a href="https://huggingface.co/papers/2506.07177">Hugging Face</a>            
          </div>
          <p> Frame Guidance is a training-free guidance method for large-scale VDMs, which enables to generate frame-level controllable videos only using a single GPU.</p>
        </div>
      </td>
    </tr>



    <tr bgcolor="#ffffd0" style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/float.gif" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          <div>
            <a href="https://deepbrainai-research.github.io/float/">
              <span class="papertitle">FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</span>
            </a>
            <br>
            <strong>Taekyung Ki</strong>,
            <a href="https://kevinmin95.github.io">Dongchan Min</a>,
            Gyeongsu Chae
            <br>
            International Conference on Computer Vision <strong style="color: navy;"><em>(ICCV)</em></strong>, 2025
            <br>
            Audio-Visual Generation & Learning Workshop at ICCV <strong style="color: navy;"><em>(AVGen@ICCV)</em></strong>, 2025
            <br>
            <a href="https://deepbrainai-research.github.io/float/">Project Page</a> /
            <a href="https://arxiv.org/abs/2412.01064">arXiv</a> /
            <a href="https://github.com/deepbrainai-research/float">Code</a> / 
            <a href="https://huggingface.co/papers/2412.01064">Hugging Face </a>            
            <p>FLOAT is a motion latent flow matching model for audio-driven talking portrait generation and editing using its learned orthonormal motion basis.</p>
          </div>
        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/export3d.gif" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:5px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://export3d.github.io">
              <span class="papertitle">Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation</span>
            </a>
            <br>
            <strong>Taekyung Ki</strong>,
            <a href="https://kevinmin95.github.io">Dongchan Min</a>,
            Gyeongsu Chae
            <br>
            European Conference on Computer Vision <strong style="color: navy;"><em>(ECCV)</em></strong>, 2024
            <br>
            <a href="https://export3d.github.io">Project Page</a> /
            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00159.pdf"> Paper</a> /
            <a href="https://arxiv.org/abs/2404.00636">arXiv</a> /
            <a href="https://fq.pkwyx.com/default/https/www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00159-supp.pdf">Supp</a>
            <p>We propose a contrastive pre-training framework for appearance-free facial expression hidden in 3DMM and 3D-aware, expression-controllable portrait animation model.</p>
          </div>
        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/stylelipsync.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="https://stylelipsync.github.io">
              <span class="papertitle">StyleLipSync: Style-based Personalized Lip-sync Video Generation</span>
            </a>
            <br>
            <strong>Taekyung Ki<sup>&ast;</sup></strong>, 
            <a href="https://kevinmin95.github.io">Dongchan Min<sup>&ast;</sup></a>
            <br>
            International Conference on Computer Vision <strong style="color: navy;"><em>(ICCV)</em></strong>, 2023
            <br>
            <a href="https://stylelipsync.github.io">Project Page</a> /
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ki_StyleLipSync_Style-based_Personalized_Lip-sync_Video_Generation_ICCV_2023_paper.pdf">Paper</a> /
            <a href="https://arxiv.org/abs/2305.00521">arXiv</a> /
            <a href="https://github.com/TaekyungKi/StyleLipSync">Code</a> /
            <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Ki_StyleLipSync_Style-based_Personalized_ICCV_2023_supplemental.pdf">Supp</a>
            <p>StyleLipSync can generate person-agnostic audio-lip synchronized videos by leveraging the strong facial prior of style-based generator.</p>
          </div>

        </div>
      </td>
    </tr>

    <tr style="height:160px;">
      <td style="padding:10px; width:32%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/publication/scattering.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:68%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          
          <div>
            <a href="">
              <span class="papertitle">Deep Scattering Network with Max-pooling</span>
            </a>
            <br>
            <strong>Taekyung Ki</strong>, 
            <a href="http://wavelets.yonsei.ac.kr/~wavelets/index.html">Youngmi Hur<sup>&dagger;</sup></a>
            <br>
            IEEE Data Compression Conference <strong style="color: navy;"><em>(DCC)</em></strong>, 2021
            <br>
            <a href="https://arxiv.org/abs/2101.02321">Paper</a>
            /
            <a href="https://github.com/TaekyungKi/Scattering_maxp">Code</a>
            <p> We mathematically prove that the pooling operator is a crucial component for translation-invariant feature extraction in Scattering Network.</p>
          </div>

        </div>
      </td>
    </tr>
  </tbody>
</table>
<br>
<table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
<tbody>
<table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
  <h2>Talks</h2>
    <tr>
      <td style="padding:10px; width:20%; vertical-align:middle;">
        <div style="height:100%; display:flex; align-items:center; justify-content:center;">
          <img src="assets/talk/talk-2508.png" style="max-height:100%; max-width:100%; object-fit:contain;" alt="Demo">
        </div>
      </td>
      <td style="padding:10px; width:80%; vertical-align:middle;">
        <div style="display:flex; flex-direction:column; justify-content:center; height:100%;">
          <div>
            <span class="title"><b>Motion Latent Flow Matching for Real-time Audio-driven Talking Portrait</b></span></a>
            <br> <a href="https://pika.art">Pika Labs</a>
            <br> Host: <a href="https://scholar.google.com/citations?user=nEFU7wIAAAAJ&hl=en">Chenlin Meng</a> (CTO)
            <br> Aug. 2025
          </div>
        </div>
      </td>
    </tr>
  </tbody>
</table>


<br><br>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
      <td style="padding:0px;width:100%;vertical-align:middle">
        <h2>Awards and Honors</h2>
        <ul>
          <li>[Oct. 2021] 1st Prize Winner, NLP-based Math-Word Problem Track in 2021 AI Grand Challenge (AGC), funded by <a href='https://www.msit.go.kr/eng/index.do'>South Korea Ministry of Science and ICT</a>.</li>
          <li>[Feb. 2019] Graduated with top honors, ranked 1st in Department of Mathematics.</li>
          </ul>
      </td>
    </tr>
  </tbody>
  </table>

<br><br>

<!--  -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
    <td style="padding:0px;width:100%;vertical-align:middle">
      <h2>Academic Services</h2>
      <ul>
        <li>Reviewer: CVPR'25-26, ICCV'25, ECCV'26, ACL ARR'26, BMVC'26</li>
    </ul>
    </td>
  </tr>
</tbody>
</table>

</tbody>
</table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Last updated in February 2026. This page is based on Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">website template</a>.
                </p>
                <br>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>